{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import mne\n",
    "from mne.io import read_raw_fif, concatenate_raws\n",
    "from mne.datasets import visual_92_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "   trigger       condition  human  face  animal  natural\n",
      "0        0  human bodypart      1     0       1        1\n",
      "1        1  human bodypart      1     0       1        1\n",
      "2        2  human bodypart      1     0       1        1\n",
      "3        3  human bodypart      1     0       1        1\n",
      "4        4  human bodypart      1     0       1        1\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "data_path = visual_92_categories.data_path()\n",
    "\n",
    "# Define stimulus - trigger mapping\n",
    "fname = op.join(data_path, 'visual_stimuli.csv')\n",
    "conds = read_csv(fname)\n",
    "print(conds.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trigger = 92\n",
    "conds = conds[:max_trigger]  # take only the first 24 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0/human bodypart/human/not-face/animal/natural', '1/human bodypart/human/not-face/animal/natural', '2/human bodypart/human/not-face/animal/natural', '3/human bodypart/human/not-face/animal/natural', '4/human bodypart/human/not-face/animal/natural']\n"
     ]
    }
   ],
   "source": [
    "conditions = []\n",
    "for c in conds.values:\n",
    "    cond_tags = list(c[:2])\n",
    "    cond_tags += [('not-' if i == 0 else '') + conds.columns[k]\n",
    "                  for k, i in enumerate(c[2:], 2)]\n",
    "    conditions.append('/'.join(map(str, cond_tags)))\n",
    "print(conditions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_id = dict(zip(conditions, conds.trigger + 1))\n",
    "event_id['0/human bodypart/human/not-face/animal/natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374 events found\n",
      "Event IDs: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93 200 222 244]\n"
     ]
    }
   ],
   "source": [
    "n_runs = 1  # 4 for full data (use less to speed up computations)\n",
    "fname = op.join(data_path, 'sample_subject_%i_tsss_mc.fif')\n",
    "raws = [read_raw_fif(fname % block, verbose='error')\n",
    "        for block in range(n_runs)]  # ignore filename warnings\n",
    "raw = concatenate_raws(raws)\n",
    "\n",
    "events = mne.find_events(raw, min_duration=.002)\n",
    "events = events[events[:, 2] <= max_trigger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "Not setting metadata\n",
      "920 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 920 events and 501 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "picks = mne.pick_types(raw.info, meg=True)\n",
    "epochs = mne.Epochs(raw, events=events, event_id=event_id, baseline=None,\n",
    "                    picks=picks, tmin=-.1, tmax=.4, preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = epochs.copy().crop(0.05, 0.3).get_data()\n",
    "Q = epochs.copy().crop(-0.1, 0).get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(920, 306, 101)\n",
      "(920, 306, 251)\n"
     ]
    }
   ],
   "source": [
    "print(Q.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Morlet Wavelet\n",
    "myfreqrange = np.array([2,120])\n",
    "Fs = 1000; # Hz, sampling rate\n",
    "#timedur = Q.shape[2]/Fs; % seconds, temporal duration\n",
    "timevec = np.linspace(0.05, 0.3, 251) # vector of time between 0 and timedur seconds\n",
    "timevec_gauss = np.linspace(-2, 2, 4001)\n",
    "lofreq = myfreqrange[0]\n",
    "hifreq = myfreqrange[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NWavelets = 236\n",
    "NCycles = 7\n",
    "MyFreqs = np.linspace(lofreq,hifreq,NWavelets)\n",
    "MyMorletFamily = np.empty((0,4001), float) #equivalent to \"clear\" # What is the equivalent of clear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normpdf(x, mu=0, sigma=1):\n",
    "    u = float((x-mu) / abs(sigma))\n",
    "    y = np.exp(-u*u/2) / (np.sqrt(2*np.pi) * abs(sigma))\n",
    "    return y\n",
    "\n",
    "for wnum in range(0, NWavelets):\n",
    "        myomega = 2 * np.pi * MyFreqs[wnum];\n",
    "        mysigma = NCycles/myomega\n",
    "        mygauss = np.array([normpdf(i,0,mysigma)\n",
    "                    for i in timevec_gauss])\n",
    "        mySig = np.exp(1j*myomega*timevec_gauss)\n",
    "        MyMorletFamily = np.append(MyMorletFamily, [mySig * mygauss], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyMorletFamily.shape\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fre_decomp(fftMEG, MyMorletFamily, NEvents, nconvolution, nshift):\n",
    "    fftGW = np.fft.fft(MyMorletFamily[:],nconvolution) # one dimensional transform?\n",
    "    fftconv = fftMEG * np.matlib.repmat(fftGW,NEvents,1) # * for multiplying numpy arrays\n",
    "    conv_result = np.fft.ifft(fftconv,nconvolution,1)\n",
    "    conv_result = conv_result[:,(nshift):(conv_result.shape[1] - nshift)] # index?  \n",
    "    power = np.power(np.absolute(conv_result), 2)\n",
    "    del fftGW \n",
    "    del fftconv\n",
    "    del conv_result\n",
    "    gc.collect()\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer time series data into frequency domain\n",
    "MEG_Power =  np.empty((0, 236,251,920))\n",
    "\n",
    "# Baseline; divide by -.1 to 0\n",
    "\n",
    "\n",
    "\n",
    "# Time Bins\n",
    "TimeBins = np.linspace(0.05, 0.3, 25)\n",
    "# Frequency Bins\n",
    "FreqBins = np.array([2, 4, 8, 13, 20, 35, 55, 80, 120])\n",
    "\n",
    "Nchan = X.shape[1]\n",
    "NTBins = len(TimeBins)\n",
    "NFBins = len(FreqBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#for chan in range(0, 5): #looping thru channels\n",
    "for chan in range(0, 1):\n",
    "#chan = 1\n",
    "    #Signal Data\n",
    "    #print(chan)\n",
    "    data = np.squeeze(X[:, chan, :])\n",
    "    NEvents = data.shape[0]\n",
    "    nconvolution = timevec_gauss.size + data.shape[1] - 1\n",
    "    nshift = int((timevec_gauss.size-1)/2)\n",
    "    \n",
    "    #Baseline Data\n",
    "    pretrial_data = np.squeeze(Q[:, chan, :])\n",
    "    pretrial_nconvolution = timevec_gauss.size + pretrial_data.shape[1] - 1\n",
    "    nshift = int((timevec_gauss.size-1)/2)\n",
    "\n",
    "        #for enum in range(0, NEvents): #looping thru events\n",
    "\n",
    "    fftMEG = np.fft.fft(data[:,:],nconvolution) # signal into frequency domain\n",
    "    pretrial_fftMEG = np.fft.fft(pretrial_data[:,:], pretrial_nconvolution) # pretrial into frequency domain\n",
    "    \n",
    "    MEG_chan =  np.empty((0,251,920))  \n",
    "    \n",
    "    for wnum in range(0, NWavelets):\n",
    "        print(wnum)\n",
    "        #fftGW = np.fft.fft(MyMorletFamily[wnum,:],nconvolution) # one dimensional transform?\n",
    "        #pretrial_fftGW = np.fft.fft(MyMorletFamily[wnum,:], pretrial_nconvolution)\n",
    "\n",
    "        #pretrial_fftconv = pretrial_fftMEG * np.matlib.repmat(pretrial_fftGW,NEvents,1)\n",
    "        #pretrial_conv_result = np.fft.ifft(pretrial_fftconv,pretrial_nconvolution,1)\n",
    "        #pretrial_conv_result = pretrial_conv_result[:,(nshift):(pretrial_conv_result.shape[1] - nshift)]\n",
    "        #pretrial_power = np.power(np.absolute(pretrial_conv_result), 2)\n",
    "\n",
    "        #fftconv = fftMEG * np.matlib.repmat(fftGW,NEvents,1) # * for multiplying numpy arrays\n",
    "        #conv_result = np.fft.ifft(fftconv,nconvolution,1)\n",
    "        #conv_result = conv_result[:,(nshift):(conv_result.shape[1] - nshift)] # index? \n",
    "        \n",
    "        pretrial_power = fre_decomp(pretrial_fftMEG, MyMorletFamily[wnum,:], NEvents, pretrial_nconvolution, nshift)\n",
    "        signal_power = fre_decomp(fftMEG, MyMorletFamily[wnum,:], NEvents, nconvolution, nshift)\n",
    "\n",
    "        normalized_MEG = np.transpose(signal_power)/np.matlib.repmat(np.median(np.transpose(pretrial_power), 0) , 251, 1)\n",
    "\n",
    "        MEG_chan = np.append(MEG_chan, [normalized_MEG], axis = 0) #numpy equivalent of transpose\n",
    "\n",
    "    MEG_Power = np.append(MEG_Power, [MEG_chan], axis = 0)\n",
    "    del MEG_chan\n",
    "    gc.collect()\n",
    "    print(chan)\n",
    "            #MEG_dB(wnum,:,:) = 10*log10(MEG_Power(wnum,:,:))\n",
    "            \n",
    "            #baseline???\n",
    "            #for loops??\n",
    "            #np.fft.fft\n",
    "            # For numpy array, it seems that * is an element wise multiplication operator. But are we operating on np.array?\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#fast fourier transform (in numpy) (cohen's book) np.fft\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#one participant, for now  \n",
    "#wavelet number, then channel number\n",
    "\n",
    "\n",
    "#fft for each channel\n",
    "# then for each wavelet, going to do the fft of the gabor wavelet family, organize it by channel\n",
    "#element by element multiplication in python: numpy dot multiply\n",
    "#conv result using ifft , second dimension (,1) in python\n",
    "#nshift gives center\n",
    "#then baseline correct the power; conv result from 50-300 and divide from -.1 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 236, 251, 920)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MEG_Power.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsearchn(x,y):\n",
    "    IDX = np.zeros((y.shape[0]))\n",
    "    for line in range(0, y.shape[0]):\n",
    "        distances = np.abs(x - y[line])\n",
    "        distances.argmin()\n",
    "        IDX[line] = distances.argmin()\n",
    "    return IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05       0.06041667 0.07083333 0.08125    0.09166667 0.10208333\n",
      " 0.1125     0.12291667 0.13333333 0.14375    0.15416667 0.16458333\n",
      " 0.175      0.18541667 0.19583333 0.20625    0.21666667 0.22708333\n",
      " 0.2375     0.24791667 0.25833333 0.26875    0.27916667 0.28958333\n",
      " 0.3       ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.05 , 0.06 , 0.071, 0.081, 0.092, 0.102, 0.112, 0.123, 0.133,\n",
       "       0.144, 0.154, 0.165, 0.175, 0.185, 0.196, 0.206, 0.217, 0.227,\n",
       "       0.237, 0.248, 0.258, 0.269, 0.279, 0.29 , 0.3  ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TimeInds = dsearchn(timevec, TimeBins)\n",
    "print(TimeBins)\n",
    "timevec[TimeInds.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-06e724093f4b>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-06e724093f4b>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    clear temp this_featureset\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Time bin + Freq bin\n",
    "AllData(pnum).EncMainEEG_dB = EncMainEEG_dB;\n",
    "\n",
    "\n",
    "\n",
    "# Create features\n",
    "TimeInds = dsearchn(timevec, TimeBins);\n",
    "FreqInds = dsearchn(MyFreqs, FreqBins);\n",
    "    \n",
    "NTBins = length(TimeInds);\n",
    "NFBins = length(FreqInds);\n",
    "AllData(pnum).MyFreqs = MyFreqs;\n",
    "AllData(pnum).TBins = TimeBins;\n",
    "AllData(pnum).FBins = FreqBins;\n",
    "    \n",
    "clear temp this_featureset\n",
    "temp = []\n",
    "this_featureset = np.zeros((NTBins, NFBins, 306, 920))\n",
    "for tnum in range(0, (NTBins-1)):\n",
    "    for fnum in range(0, (NFBins-1)):\n",
    "        temp = MEG_Power[:, FreqInds[fnum]:FreqInds[fnum + 1]]\n",
    "        temp = AllData(pnum).EncMainEEG_dB(FreqInds(fnum):FreqInds(fnum+1),...\n",
    "            TimeInds(tnum):TimeInds(tnum+1),:,:);\n",
    "        this_featureset(fnum,tnum,:,:) = zscore(median(squeeze(median(temp,1)),1));\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "AllData(pnum).EncMainEEGFeatures = this_featureset;\n",
    "    \n",
    "sprintf('Finished participant %d',pnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Classify using the average signal in the window 50ms to 300ms\n",
    "# to focus the classifier on the time interval with best SNR.\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    LogisticRegression(C=1, solver='liblinear',\n",
    "                                       multi_class='auto'))\n",
    "\n",
    "#y = epochs.events[:, 2]\n",
    "y_sup = (epochs.events[:, 2] > 48).astype(int) #set up superordinate classification label. \n",
    "classes = set(y_sup) \n",
    "\n",
    "#cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "# Compute confusion matrix for each cross-validation fold\n",
    "#y_pred = np.zeros((len(y_sup), len(classes)))\n",
    "#for train, test in cv.split(X, y_sup):\n",
    "    # Fit\n",
    " #   clf.fit(X[train], y_sup[train])\n",
    "    # Probabilistic prediction (necessary for ROC-AUC scoring metric)\n",
    "  #  y_pred[test] = clf.predict_proba(X[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-18e5d6c61204>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_sup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_sup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "y_sup = (epochs.events[:, 2] > 48).astype(int)\n",
    "y_sup.shape\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data trianing testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regularized Regression Lasso / Rigid / Logistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67340695, 0.32659305],\n",
       "       [0.32659305, 0.67340695]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion = np.zeros((len(classes), len(classes)))\n",
    "for ii, train_class in enumerate(classes):\n",
    "    for jj in range(ii, len(classes)):\n",
    "        confusion[ii, jj] = roc_auc_score(y_sup == train_class, y_pred[:, jj])\n",
    "        confusion[jj, ii] = confusion[ii, jj]\n",
    "confusion  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
