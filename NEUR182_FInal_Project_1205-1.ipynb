{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving human object recognition: A time-frequency analysis\n",
    "\n",
    "Vani, Ben, & Jeanne\n",
    "\n",
    "NEUR182: Machine Learning w/ Neural Signal\n",
    "\n",
    "Prof. Michael Spezio\n",
    "\n",
    "Fall 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import gc # for memory cleaning\n",
    "\n",
    "from sklearn import preprocessing, model_selection, linear_model, metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import scale as CenterScale\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import mne\n",
    "from mne.io import read_raw_fif, concatenate_raws\n",
    "from mne.datasets import visual_92_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function definitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs frequency decomposition\n",
    "# returns power as np.ndarray\n",
    "\n",
    "def fre_decomp(fftMEG, MorletFamily, nEvents, nConvolution, nShift):\n",
    "    fftGW = np.fft.fft(MorletFamily[:], nConvolution)\n",
    "    fftconv = fftMEG * np.matlib.repmat(fftGW, nEvents, 1)\n",
    "    conv_result = np.fft.ifft(fftconv, nConvolution, 1)\n",
    "    conv_result = conv_result[:, (nShift):(conv_result.shape[1] - nShift)]\n",
    "    power = np.power(np.absolute(conv_result), 2)\n",
    "    \n",
    "    del fftGW \n",
    "    del fftconv\n",
    "    del conv_result\n",
    "    gc.collect()\n",
    "    \n",
    "    return power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements Matlab's dsearchn()\n",
    "# returns indices of nearest points as np.ndarray\n",
    "\n",
    "def dsearchn(x, y):\n",
    "    dist = np.zeros((y.shape[0]), int)\n",
    "    \n",
    "    for line in range(0, y.shape[0]):\n",
    "        distances = np.abs(x - y[line])\n",
    "        distances.argmin()\n",
    "        dist[line] = distances.argmin().astype(int)\n",
    "    \n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements Matlab's normpdf()\n",
    "# returns pdf as float\n",
    "\n",
    "def normpdf(x, mu=0, sigma=1):\n",
    "    u = float((x-mu) / abs(sigma))\n",
    "    y = np.exp(-u*u/2) / (np.sqrt(2*np.pi) * abs(sigma))\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs feature engineering\n",
    "# returns feature set as np.ndarray\n",
    "\n",
    "def FeatureBins(MEG_chan, timeIndx, freqIndx, nEvents):\n",
    "    nTBins = len(timeIndx)\n",
    "    nFBins = len(freqIndx)\n",
    "    featureset = np.zeros((nEvents, nFBins-1, nTBins-1))\n",
    "   \n",
    "    for time in range (0, nTBins-1):\n",
    "        for freq in range (0, nFBins-1):\n",
    "            temp = MEG_chan[(freqIndx[freq]):(freqIndx[freq + 1]), \n",
    "                            (timeIndx[time]):(timeIndx[time + 1]),:]\n",
    "            featureset[:, freq, time] = stats.zscore(np.median(np.squeeze(np.median(temp,0)),0))\n",
    "     \n",
    "    return featureset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunes regression models using validation proportion of training set\n",
    "# returns \n",
    "\n",
    "def TuningModels(models, X, Y, val_size, iterations = 100):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        metric_train = []\n",
    "        metric_val = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X,Y,test_size = val_size)\n",
    "            metric_val.append(metrics.mean_squared_error(y_val,\n",
    "                                                     models[i].fit(X_train,\n",
    "                                                                   y_train).predict(X_val)))\n",
    "            metric_train.append(metrics.mean_squared_error(y_train,\n",
    "                                                       models[i].fit(X_train, \n",
    "                                                                     y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(metric_train), np.mean(metric_val)]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[begin data shaping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = visual_92_categories.data_path()\n",
    "\n",
    "# define stimulus-trigger mapping (code from King, Leppakangas, & Gramfort)\n",
    "fname = op.join(data_path, 'visual_stimuli.csv')\n",
    "conds = read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 92 rows\n",
    "max_trigger = 92\n",
    "conds = conds[:max_trigger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = []\n",
    "for c in conds.values:\n",
    "    cond_tags = list(c[:2])\n",
    "    cond_tags += [('not-' if i == 0 else '') + conds.columns[k]\n",
    "                  for k, i in enumerate(c[2:], 2)]\n",
    "    conditions.append('/'.join(map(str, cond_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = dict(zip(conditions, conds.trigger + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374 events found\n",
      "Event IDs: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93 200 222 244]\n"
     ]
    }
   ],
   "source": [
    "nRuns = 1  # 4 for full data (use less to speed up computations)\n",
    "fname = op.join(data_path, 'sample_subject_%i_tsss_mc.fif')\n",
    "\n",
    "raws = [read_raw_fif(fname % block, verbose='error')\n",
    "        for block in range(0, nRuns)]  # ignore filename warnings\n",
    "raw = concatenate_raws(raws)\n",
    "\n",
    "events = mne.find_events(raw, min_duration=.002)\n",
    "events = events[events[:, 2] <= max_trigger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "Not setting metadata\n",
      "920 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 920 events and 501 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "picks = mne.pick_types(raw.info, meg=True)\n",
    "epochs = mne.Epochs(raw, events=events, event_id=event_id, baseline=None,\n",
    "                    picks=picks, tmin=-.1, tmax=.4, preload=True)\n",
    "del raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual representation of epochs data structure\n",
    "#epochs.plot(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep50_300 = epochs.copy().crop(0.05, 0.3).get_data()\n",
    "base10_0 = epochs.copy().crop(-0.1, 0).get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[end data shaping] [begin data processing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize elements for Morlet wavelets\n",
    "nWavelets = 236\n",
    "freq_range = np.array([2,120])\n",
    "lofreq = freq_range[0]\n",
    "hifreq = freq_range[1]\n",
    "freqs = np.linspace(lofreq, hifreq, nWavelets)\n",
    "Fs = 1000 # Hz, sampling rate\n",
    "timevec = np.linspace(0.05, 0.3, 251)\n",
    "timevec_gauss = np.linspace(-2, 2, 4001)\n",
    "nCycles = 7\n",
    "\n",
    "MorletFamily = np.empty((0, 4001), float)\n",
    "\n",
    "# create complex wavelet family\n",
    "for wavelet in range(0, nWavelets):\n",
    "        omega = 2 * np.pi * freqs[wavelet];\n",
    "        sigma = nCycles / omega\n",
    "        gauss = np.array([normpdf(i,0, sigma)\n",
    "                    for i in timevec_gauss])\n",
    "        sig = np.exp(1j * omega * timevec_gauss)\n",
    "        MorletFamily = np.append(MorletFamily, [sig * gauss], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time indices are:\n",
      "[  0  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170\n",
      " 180 190 200 210 220 230 240 250]\n",
      "\n",
      "frequency indices are:\n",
      "[  0   4  12  22  36  66 106 155 235]\n"
     ]
    }
   ],
   "source": [
    "# initalize elements for transferring time series to frequency domain\n",
    "nEvents = ep50_300.shape[0]\n",
    "MEG_Power = np.zeros((nEvents,306,8,25))\n",
    "nChan = ep50_300.shape[1]\n",
    "nShift = int((timevec_gauss.size-1)/2)\n",
    "\n",
    "# time & frequency bins\n",
    "TimeBins = np.linspace(0.05, 0.3, 26)\n",
    "FreqBins = np.array([2, 4, 8, 13, 20, 35, 55, 80, 120])\n",
    "timeIndx = dsearchn(timevec, TimeBins)\n",
    "freqIndx = dsearchn(freqs, FreqBins)\n",
    "\n",
    "print(\"time indices are:\") \n",
    "print(timeIndx)\n",
    "print(\"\\nfrequency indices are:\")\n",
    "print(freqIndx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0d3f7d97efb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwavelet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnWavelets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         pre_power = fre_decomp(pre_fftMEG, MorletFamily[wavelet,:], nEvents, \n\u001b[0;32m---> 23\u001b[0;31m                                pre_nConvolution, nShift)\n\u001b[0m\u001b[1;32m     24\u001b[0m         sig_power = fre_decomp(fftMEG, MorletFamily[wavelet,:], nEvents, \n\u001b[1;32m     25\u001b[0m                                nConvolution, nShift)\n",
      "\u001b[0;32m<ipython-input-2-64cbd6650328>\u001b[0m in \u001b[0;36mfre_decomp\u001b[0;34m(fftMEG, MorletFamily, nEvents, nConvolution, nShift)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mfftconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mconv_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# *** loop over channels ***\n",
    "for chan in range(0, nChan): \n",
    "\n",
    "    # process baseline data\n",
    "    pre_data = np.squeeze(base10_0[:,chan,:])\n",
    "    pre_nConvolution = timevec_gauss.size + pre_data.shape[1] - 1\n",
    "    \n",
    "    # process signal data\n",
    "    data = np.squeeze(ep50_300[:,chan,:])\n",
    "    nEvents = data.shape[0]\n",
    "    nConvolution = timevec_gauss.size + data.shape[1] - 1\n",
    "\n",
    "    # convert signal & baseline to frequency domain\n",
    "    pre_fftMEG = np.fft.fft(pre_data[:,:], pre_nConvolution)\n",
    "    fftMEG = np.fft.fft(data[:,:], nConvolution)\n",
    "\n",
    "    MEG_chan =  np.empty((0, 251, nEvents))  \n",
    "\n",
    "    \n",
    "    # ****** loop over wavelets *******\n",
    "    for wavelet in range(0, nWavelets):\n",
    "        pre_power = fre_decomp(pre_fftMEG, MorletFamily[wavelet,:], nEvents, \n",
    "                               pre_nConvolution, nShift)\n",
    "        sig_power = fre_decomp(fftMEG, MorletFamily[wavelet,:], nEvents, \n",
    "                               nConvolution, nShift)\n",
    "\n",
    "        normal_MEG = (np.transpose(sig_power) / \n",
    "                      np.matlib.repmat(np.median(np.transpose(pre_power), 0), 251, 1))\n",
    "        MEG_chan = np.append(MEG_chan, [normal_MEG], axis = 0)\n",
    "       \n",
    "    \n",
    "    # store feature set for current electrode\n",
    "    MEG_chanBins = FeatureBins(MEG_chan, timeIndx, freqIndx, nEvents)      \n",
    "    MEG_Power[:,chan,:,:] = MEG_chanBins  # [electrode x trial x wavelet x time]\n",
    "    \n",
    "    del MEG_chan\n",
    "    del MEG_chanBins\n",
    "    gc.collect()     \n",
    "    \n",
    "    print(chan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[end data processing] [begin data analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 306, 8, 25)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data features\n",
    "featureset = np.load('Pt1Featureset.npy')\n",
    "featureset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(920,)\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "# create labels\n",
    "y_sup = (epochs.events[:, 2] > 48).astype(int) # set up superordinate classification label\n",
    "print(y_sup.shape)\n",
    "\n",
    "classes = set(y_sup) \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "Feats_train, Feats_test, OV_train, OV_test = train_test_split(featureset, y_sup, test_size=0.3, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Feats_train', feats_train)\n",
    "np.save('Feats_test', feats_test)\n",
    "np.save('OV_train', OV_train)\n",
    "np.save('OV_test', OV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feats_train = np.load('Feats_train.npy')\n",
    "Feats_test = np.load('Feats_test.npy')\n",
    "OV_train = np.load('OV_train.npy')\n",
    "OV_test = np.load('OV_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644, 61200)\n",
      "(276, 61200)\n"
     ]
    }
   ],
   "source": [
    "print(Feats_train.shape)\n",
    "print(Feats_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feats_train = np.reshape(Feats_train, (644, 61200))\n",
    "Feats_test = np.reshape(Feats_test, (276, 61200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644, 61200)\n",
      "(276, 61200)\n"
     ]
    }
   ],
   "source": [
    "print(Feats_train.shape)\n",
    "print(Feats_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assess correlation between features to check for multicollinearity\n",
    "# PVsCorrMat = np.corrcoef(feats_train, rowvar = False)\n",
    "# PVsCoDMat = np.power(PVsCorrMat, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(PVsCodDMat.shape)\n",
    "# print(PVsCodDMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import matplotlib.cm as cm\n",
    "#surf(PVsCoDMat,cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components = 2)\n",
    "#X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[regularized regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit lasso & ridge models with a tuning approach over hyperparameter α (λ)\n",
    "\n",
    "lasso_params = {'alpha':np.linspace(1e-5,1e-2,30)}\n",
    "ridge_params = {'alpha':np.linspace(1e-10,20,50)}\n",
    "\n",
    "models = {'Lasso': GridSearchCV(linear_model.Lasso(tol = 1e-3, random_state=47), \n",
    "                                param_grid=lasso_params).fit(Feats_train, OV_train).best_estimator_,\n",
    "          'Ridge': GridSearchCV(linear_model.Ridge(tol = 1e-3, random_state = 47),\n",
    "                                 param_grid=ridge_params).fit(Feats_train, OV_train).best_estimator_}\n",
    "\n",
    "reg_result = TuningModels(models, Feats_train, OV_train, val_size = 0.2, iterations = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lasso': [0.012464100328162519, 0.3018793766747705], 'Ridge': [7.098829730860406e-08, 0.2628597931900838]}\n",
      "Optimal alpha for LASSO is 0.01\n",
      "Optimal alpha for Ridge is 20.0\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print(reg_result)\n",
    "\n",
    "# print optimal parameters after tuning\n",
    "print('Optimal alpha for LASSO is ' + str(models['Lasso'].alpha))\n",
    "print('Optimal alpha for Ridge is ' + str(models['Ridge'].alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized regression -- lasso\n",
    "LassoModel = Lasso(tol = 1e-3, random_state = 47, alpha = models['Lasso'].alpha)\n",
    "LassoModel.fit(Feats_train, OV_train)\n",
    "LassoPredictTest = LassoModel.predict(Feats_test)\n",
    "\n",
    "# calculate performance metrics\n",
    "LassoRSqError = 1 - metrics.explained_variance_score(OV_test, LassoPredictTest)\n",
    "print('Lasso Rsq Error = ' + str(LassoRSqError))\n",
    "Lasso_b_acc = metrics.balanced_accuracy_score(OV_test, LassoPredictTest)\n",
    "print('Lasso balanced accuracy = ' + str(Lasso_b_acc))\n",
    "Lasso_roc_auc = metrics.roc_auc_score(OV_test, LassoPredictTest)\n",
    "print('Lasso ROC/AUC = ' + str(Lasso_roc_auc))\n",
    "Lasso_f1 = metrics.f1_score(OV_test, LassoPredictTest)\n",
    "print('Lasso F1 = ' + str(Lasso_f1)\n",
    "Lasso_precision = metrics.precision_score(OV_test, LassoPredictTest)\n",
    "print('Lasso precision = ' + str(Lasso_precision)\n",
    "Lasso_recall = metrics.recall_score(OV_test, LassoPredictTest)\n",
    "print('Lasso recall = ' + str(Lasso_recall)\n",
    "Lasso_confused = confusion_matrix(OV_test, LassoPredictTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Rsq Error = 1.1323441531983698\n",
      "Ridge Rsq Error = 1.1733581292235875\n"
     ]
    }
   ],
   "source": [
    "# regularized regression -- ridge\n",
    "RidgeModel = Ridge(tol = 1e-3, random_state = 47, alpha = models['Ridge'].alpha)\n",
    "RidgeModel.fit(Feats_train, OV_train)\n",
    "RidgePredictTest = RidgeModel.predict(Feats_test)\n",
    "\n",
    "# calculate performace metrics\n",
    "RidgeRSqError = 1 - metrics.explained_variance_score(OV_test, RidgePredictTest)\n",
    "print('Ridge Rsq Error = ' + str(RidgeRSqError))\n",
    "Ridge_b_acc = metrics.balanced_accuracy_score(OV_test, RidgePredictTest)\n",
    "print('Ridge balanced accuracy = ' + str(Ridge_b_acc))\n",
    "Ridge_roc_auc = metrics.roc_auc_score(OV_test, RidgePredictTest)\n",
    "print('Ridge ROC/AUC = ' + str(Ridge_roc_auc))\n",
    "Ridge_f1 = metrics.f1_score(OV_test, RidgePredictTest)\n",
    "print('Ridge F1 = ' + str(Ridge_f1)\n",
    "Ridge_precision = metrics.precision_score(OV_test, RidgePredictTest)\n",
    "print('Ridge precision = ' + str(Ridgeo_precision)\n",
    "Ridge_recall = metrics.recall_score(OV_test, RidgePredictTest)\n",
    "print('Ridge recall = ' + str(Ridge_recall)\n",
    "Ridge_confused = confusion_matrix(OV_test, RidgePredictTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SVM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# svm tuning\n",
    "def svc_tune(X, y, nfolds, random_state=10):\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "                     'C': [ 0.1, 10, 100, 1000]}\n",
    "                    #{'kernel': ['sigmoid'], 'gamma': [1e-5, 1e-6, 1e-7, 1e-8],\n",
    "                     #'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}\n",
    "                    #{'kernel': ['linear'], 'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}\n",
    "                   ]\n",
    "    grid_search = GridSearchCV(SVC(C=1), tuned_parameters, scoring='roc_auc', cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = svc_tune(CenterScale(Feats_train), OV_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.0005, kernel='sigmoid',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "# Specify parameters\n",
    "kernel_choice = 'sigmoid'\n",
    "\n",
    "# Specify our object\n",
    "MySVC = SVC(kernel = kernel_choice, C = 10, gamma =0.0005)\n",
    "\n",
    "# Train our SVC\n",
    "MySVC.fit(CenterScale(feats_train),OV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.5112964527027027\n",
      "Balanced Accuracy = 0.5112964527027027\n",
      "F1 = 0.46692607003891046\n"
     ]
    }
   ],
   "source": [
    "TestPred = MySVC.predict(CenterScale(feats_test))\n",
    "AUCResult = metrics.roc_auc_score(OV_test,TestPred)\n",
    "print('AUC = ' + str(AUCResult))\n",
    "BalAccResult = metrics.balanced_accuracy_score(OV_test,TestPred)\n",
    "print('Balanced Accuracy = ' + str(BalAccResult))\n",
    "F1Result = metrics.f1_score(OV_test,TestPred)\n",
    "print('F1 = ' + str(F1Result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "# Specify parameters\n",
    "kernel_choice = 'linear'\n",
    "\n",
    "# Specify our object\n",
    "MyLinearSVC = SVC(kernel = kernel_choice, C = 10)\n",
    "\n",
    "# Train our SVC\n",
    "MyLinearSVC.fit(CenterScale(feats_train),OV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.5203758445945946\n",
      "Balanced Accuracy = 0.5203758445945946\n",
      "F1 = 0.4924242424242424\n"
     ]
    }
   ],
   "source": [
    "TestPred = MyLinearSVC.predict(CenterScale(feats_test))\n",
    "AUCResult = metrics.roc_auc_score(OV_test,TestPred)\n",
    "print('AUC = ' + str(AUCResult))\n",
    "BalAccResult = metrics.balanced_accuracy_score(OV_test,TestPred)\n",
    "print('Balanced Accuracy = ' + str(BalAccResult))\n",
    "F1Result = metrics.f1_score(OV_test,TestPred)\n",
    "print('F1 = ' + str(F1Result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RVM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ussha\\anaconda3\\envs\\Neuro182_tf2\\lib\\site-packages\\skrvm\\rvm.py:239: RuntimeWarning: divide by zero encountered in log\n",
      "  np.sum(np.log(1-y[t == 0]), 0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RVC(alpha=1e-06, beta=1e-06, beta_fixed=False, bias_used=True, coef0=0.0,\n",
       "    coef1=None, degree=3, kernel='linear', n_iter=3000, n_iter_posterior=50,\n",
       "    threshold_alpha=1000000000.0, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skrvm import RVC\n",
    "# Specify parameters\n",
    "kernel_choice = 'linear'\n",
    "\n",
    "# Specify our object\n",
    "MyRVC = RVC(kernel = kernel_choice, verbose=False)\n",
    "\n",
    "# Train our RVC\n",
    "from sklearn.preprocessing import scale as CenterScale\n",
    "MyRVC.fit(CenterScale(feats_train),OV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Relevance Vectors = 105\n",
      "Median of Feature Weights (only available for Linear Kernel) = -0.19561504771603905\n",
      "Maximum of Feature Weights (only available for Linear Kernel) = 25.30679633714632\n"
     ]
    }
   ],
   "source": [
    "# Explore our classifier\n",
    "print('Number of Relevance Vectors = ' + str(MyRVC.relevance_.shape[0]))\n",
    "print('Median of Feature Weights (only available for Linear Kernel) = ' + str(np.median(MyRVC.relevance_)))\n",
    "print('Maximum of Feature Weights (only available for Linear Kernel) = ' + str(np.max(MyRVC.relevance_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.5008445945945946\n",
      "Balanced Accuracy = 0.5008445945945946\n",
      "F1 = 0.48327137546468396\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "from sklearn import metrics\n",
    "TestPred = MyRVC.predict(CenterScale(feats_test))\n",
    "AUCResult = metrics.roc_auc_score(OV_test,TestPred)\n",
    "print('AUC = ' + str(AUCResult))\n",
    "BalAccResult = metrics.balanced_accuracy_score(OV_test,TestPred)\n",
    "print('Balanced Accuracy = ' + str(BalAccResult))\n",
    "F1Result = metrics.f1_score(OV_test,TestPred)\n",
    "print('F1 = ' + str(F1Result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[random forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RF\n",
    "\n",
    "ncores = 2\n",
    "\n",
    "\n",
    "# Grid of hyperparameters\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in numpy.linspace(start = 20, stop = 2000, num = 50)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in numpy.linspace(10, 110, num=11)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split}\n",
    "    \n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100, criterion = 'gini', max_depth = 20,  min_samples_split = 10, max_features = 'sqrt')\n",
    "forest.fit(Feats_train, OV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid Search (This is run elsewhere. See the next cell for results)\n",
    "grid_search = GridSearchCV(estimator = forest, param_grid = random_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(Feats_train, OV_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'max_depth': 20,\n",
    " 'max_features': 'sqrt',\n",
    " 'min_samples_split': 10,\n",
    " 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Prediction\n",
    "forest_predict_train = forest.predict(Feats_train)\n",
    "\n",
    "# Best Parameter Evaluation\n",
    "params_eval = f1_score(OV_train, forest_prediction_train)\n",
    "\n",
    "# Testing Prediction \n",
    "OV_pred = forest.predict(Feats_test)\n",
    "\n",
    "# Performance Metrics\n",
    "b_accuracy = balanced_accuracy_score(OV_test, OV_pred)\n",
    "print(b_accuracy)\n",
    "roc_auc = roc_auc_score(OV_test, OV_pred)\n",
    "print(roc_auc)\n",
    "f1 = f1_score(OV_test, OV_pred)\n",
    "print(f1)\n",
    "precision = precision_score(OV_test, OV_pred)\n",
    "print(precision)\n",
    "recall = recall_score(OV_test, OV_pred)\n",
    "print(recall)\n",
    "confused = confusion_matrix(OV_test, OV_pred)\n",
    "print(confused)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from Performance metrics:\n",
    "Balanced Accuracy =.5461359797297297\n",
    "ROC AUC = 0.5461359797297297\n",
    "F1 = 0.3195876288659794\n",
    "Precision = 0.6739130434782609\n",
    "Recall = 0.20945945945945946"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
